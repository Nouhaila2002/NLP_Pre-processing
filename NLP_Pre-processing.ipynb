{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c22dbe",
   "metadata": {},
   "source": [
    "# NLP: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95f72f",
   "metadata": {},
   "source": [
    "**In NLP problems it is usual to make the text pass first into a preprocessing pipeline. So to all techniques used to transform the text into embeddings, the texts were first preprocessed using the following steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4610d",
   "metadata": {},
   "source": [
    "**Normalization:** transforming the text into lower case and removing all the special characters and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c19074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      " Modeling, is arguably the most fun part of a machine learning task. #ML https://data.com\n",
      "after:\n",
      " Modeling, is arguably the most fun part of a machine learning task. ML \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "text =  \"Modeling, is arguably the most fun part of a machine learning task. #ML https://data.com\"\n",
    "\n",
    "print(\"before:\\n\",text)\n",
    "def cleaning(text):\n",
    "    #remove the #\n",
    "    clean_text = re.sub(r'#',\"\",text)\n",
    "    #remove hyperlinks\n",
    "    clean_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*',\"\",clean_text)\n",
    "    #remove retweet text\n",
    "    clean_text = re.sub(r'^RT[\\s]+',\"\",clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "print(\"after:\\n\",cleaning(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb183b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      " Modeling, is arguably the { most fun part of a  machine learning task.\n",
      "after\n",
      " ['Modeling,', 'is', 'arguably', 'the', 'most', 'fun', 'part', 'of', 'a', 'machine', 'learning', 'task.']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "    \n",
    "text =  \"Modeling, is arguably the { most fun part of a \"\" machine learning task.\"\n",
    "\n",
    "print(\"before:\\n\",text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    clean_text = []\n",
    "    #remove punctuations\n",
    "    for word in text.split():\n",
    "        if(word not in string.punctuation):\n",
    "            clean_text.append(word)\n",
    "            \n",
    "    return clean_text\n",
    "\n",
    "print(\"after\\n\", remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd6b4e",
   "metadata": {},
   "source": [
    "**Removing stop words:** stop words are the words that are most commonly used in a language and do not add much meaning to the text. Some examples are the words ‘the’, ‘a’, ‘will’,…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28dca980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      " Modeling, is arguably the most fun part of a  machine learning task.\n",
      "after\n",
      " ['Modeling,', 'arguably', 'fun', 'part', 'machine', 'learning', 'task.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import the list of english stopwords from nltk\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "text =  \"Modeling, is arguably the most fun part of a  machine learning task.\"\n",
    "\n",
    "print(\"before:\\n\",text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    clean_text = []\n",
    "    #remove punctuations\n",
    "    for word in text.split():\n",
    "        if(word not in eng_stopwords):\n",
    "            clean_text.append(word)\n",
    "            \n",
    "    return clean_text\n",
    "\n",
    "print(\"after\\n\", remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f0a02",
   "metadata": {},
   "source": [
    "**Tokenization:** getting the normalized text and splitting it into a list of tokens.\n",
    "\n",
    "**String Tokenization:** In Natural Language Processing, String Tokenization is a process where the string is splitted into Individual words or Individual parts without blanks and tabs. In the same step, the words in the String is converted into lower case. The Tokenize Module from NLTK or Naural Language Toolkit makes very easy to carry out this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d18864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      " Modeling, is arguably the MOST fun part of a  Machine learning task.\n",
      "after\n",
      " ['modeling', ',', 'is', 'arguably', 'the', 'most', 'fun', 'part', 'of', 'a', 'machine', 'learning', 'task', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "text =  \"Modeling, is arguably the MOST fun part of a  Machine learning task.\"\n",
    "print(\"before:\\n\",text)\n",
    "\n",
    "#instantiate the Tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "#tokenize the text\n",
    "text_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"after\\n\",text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fc62c",
   "metadata": {},
   "source": [
    "**Stemming:** it is the process to get the root of the words and sometimes this root is not equal to the morphological root of the word, but the stemming goal is to make that related word maps to the same stem. Examples: branched and branching become branch.\n",
    "\n",
    "**Stemming in Natural Language Processing:** Stemming is a process of converting a word to its most General form or Stem. It's basically the process of removing the suffix from a word and reduce it to its root word. It helps in reducing the size of Vocabulary. It is one of the most important steps while working with Text. \n",
    "\n",
    "**Porter Stemmer:** It is one of the most common and gentle stemmer which is very fast but not very precise.\n",
    "\n",
    "**Snowball Stemmer:** It's actual name is English Stemmer is more precise over large Dataset.\n",
    "\n",
    "**Lancaster Stemmer:** It is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e58aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['modeling,', 'is', 'arguabl', 'the', 'most', 'fun', 'part', 'of', 'a', 'machin', 'learn', 'task.']\n",
      "['modeling,', 'is', 'arguabl', 'the', 'most', 'fun', 'part', 'of', 'a', 'machin', 'learn', 'task.']\n",
      "['modeling,', 'is', 'argu', 'the', 'most', 'fun', 'part', 'of', 'a', 'machin', 'learn', 'task.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "\n",
    "txt =  \"Modeling, is arguably the MOST fun part of a  Machine learning task.\"\n",
    "text = txt.split()\n",
    "\n",
    "#Porter Stemmer\n",
    "stemmer = PorterStemmer()   #instantiate stemmer class\n",
    "stemWords = [stemmer.stem(word) for word in text]\n",
    "print(stemWords)\n",
    "\n",
    "#Snowball Stemmer\n",
    "stemmer = SnowballStemmer(\"english\")   #instantiate stemmer class\n",
    "stemWords = [stemmer.stem(word) for word in text]\n",
    "print(stemWords)\n",
    "\n",
    "#Lancaster Stemmer\n",
    "stemmer = LancasterStemmer()   #instantiate stemmer class\n",
    "stemWords = [stemmer.stem(word) for word in text]\n",
    "print(stemWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c8ec2",
   "metadata": {},
   "source": [
    "**Lemmatization:** This is the process of getting the same word for a group of inflected word forms, the simplest way to do this is with a dictionary. Examples: is, was, were become be.\n",
    "\n",
    "**Lemmatization in Natural Language Processing:** Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word's Lemma or a Dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than Simple Stemming. Lemmatization looks at the surrounding text to determine a given words's part of speech where it doesn't categorize the phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aa02040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==2.3.7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'C:\\\\Users\\\\Meryem\\\\anaconda3\\\\Lib\\\\site-packages\\\\~pacy\\\\attrs.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached spacy-2.3.7-cp38-cp38-win_amd64.whl (9.7 MB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (0.7.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (3.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (2.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (1.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (2.25.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (1.20.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from spacy==2.3.7) (7.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\meryem\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7) (2.10)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb6e39ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with SpaCy: ['drink']\n",
      "Lemmatization with SpaCy: ['were']\n",
      "Lemmatization with SpaCy: ['dog']\n",
      "Lemmatization with SpaCy: ['i', 'be']\n",
      "Lemmatization with SpaCy: ['sing']\n",
      "Lemmatization with NLTK: drink\n",
      "Lemmatization with NLTK: were\n",
      "Lemmatization with NLTK: dog\n",
      "Lemmatization with NLTK: is\n",
      "Lemmatization with NLTK: sings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Meryem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "words = [\"drinks\",\"were\",\"dogs\",\"is\",\"sings\"]\n",
    "\n",
    "#lemmatization with SpaCy\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\":[['s','']]})\n",
    "lemmatizer = Lemmatizer(lookups)\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer(word, \"NOUN\")\n",
    "    print(f\"Lemmatization with SpaCy: {lemma}\")\n",
    "    \n",
    "\n",
    "#lemmatization with NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"Lemmatization with NLTK: {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3621d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with SpaCy: ['be']\n",
      "Lemmatization with SpaCy: ['be']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "words = [\"is\",\"were\"]\n",
    "\n",
    "#lemmatization with SpaCy\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\":[['is','be'],['were','be']]})\n",
    "lemmatizer = Lemmatizer(lookups)\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer(word, \"NOUN\")\n",
    "    print(f\"Lemmatization with SpaCy: {lemma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68203c",
   "metadata": {},
   "source": [
    "**The output of this pipeline is a list with the formatted tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
